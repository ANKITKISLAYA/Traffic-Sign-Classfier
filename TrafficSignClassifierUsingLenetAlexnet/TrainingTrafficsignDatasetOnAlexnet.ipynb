{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Function to perform convolution operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
    "    \n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i % group == 0\n",
    "    assert c_o % group == 0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "\n",
    "    if tf.__version__ < \"1.0.0\":\n",
    "        if group == 1:\n",
    "            conv = convolve(input, kernel)\n",
    "        else:\n",
    "            input_groups = tf.split(3, group, input)\n",
    "            kernel_groups = tf.split(3, group, kernel)\n",
    "            output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "            conv = tf.concat(3, output_groups)\n",
    "    else:\n",
    "        if group == 1:\n",
    "            conv = convolve(input, kernel)\n",
    "        else:\n",
    "            input_groups = tf.split(input, group, 3)\n",
    "            kernel_groups = tf.split(kernel, group, 3)\n",
    "            output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "            conv = tf.concat(output_groups, 3)\n",
    "    conv_return = tf.reshape(tf.nn.bias_add(conv, biases), [-1] + conv.get_shape().as_list()[1:])\n",
    "    return conv_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet Function for creating alexnet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlexNet(features, feature_extract=False):\n",
    "    \"\"\"\n",
    "    Builds an AlexNet model, loads pretrained weights\n",
    "    \"\"\"\n",
    "    net_data = np.load(\"bvlc-alexnet.npy\", allow_pickle=True, encoding=\"latin1\").item()\n",
    "    # conv1\n",
    "    # conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "    k_h = 11\n",
    "    k_w = 11\n",
    "    c_o = 96\n",
    "    s_h = 4\n",
    "    s_w = 4\n",
    "    conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "    conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "    conv1_in = conv(features, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "    conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "    # lrn1\n",
    "    # lrn(2, 2e-05, 0.75, name='norm1')\n",
    "    radius = 2\n",
    "    alpha = 2e-05\n",
    "    beta = 0.75\n",
    "    bias = 1.0\n",
    "    lrn1 = tf.nn.local_response_normalization(conv1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "\n",
    "    # maxpool1\n",
    "    # max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "    k_h = 3\n",
    "    k_w = 3\n",
    "    s_h = 2\n",
    "    s_w = 2\n",
    "    padding = 'VALID'\n",
    "    maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    # conv2\n",
    "    # conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "    k_h = 5\n",
    "    k_w = 5\n",
    "    c_o = 256\n",
    "    s_h = 1\n",
    "    s_w = 1\n",
    "    group = 2\n",
    "    conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "    conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "    conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv2 = tf.nn.relu(conv2_in)\n",
    "\n",
    "    # lrn2\n",
    "    # lrn(2, 2e-05, 0.75, name='norm2')\n",
    "    radius = 2\n",
    "    alpha = 2e-05\n",
    "    beta = 0.75\n",
    "    bias = 1.0\n",
    "    lrn2 = tf.nn.local_response_normalization(conv2, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "\n",
    "    # maxpool2\n",
    "    # max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "    k_h = 3\n",
    "    k_w = 3\n",
    "    s_h = 2\n",
    "    s_w = 2\n",
    "    padding = 'VALID'\n",
    "    maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    # conv3\n",
    "    # conv(3, 3, 384, 1, 1, name='conv3')\n",
    "    k_h = 3\n",
    "    k_w = 3\n",
    "    c_o = 384\n",
    "    s_h = 1\n",
    "    s_w = 1\n",
    "    group = 1\n",
    "    conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "    conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "    conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "\n",
    "    # conv4\n",
    "    # conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "    k_h = 3\n",
    "    k_w = 3\n",
    "    c_o = 384\n",
    "    s_h = 1\n",
    "    s_w = 1\n",
    "    group = 2\n",
    "    conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "    conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "    conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "    # conv5\n",
    "    # conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "    k_h = 3\n",
    "    k_w = 3\n",
    "    c_o = 256\n",
    "    s_h = 1\n",
    "    s_w = 1\n",
    "    group = 2\n",
    "    conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "    conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "    conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv5 = tf.nn.relu(conv5_in)\n",
    "\n",
    "\n",
    "    # maxpool5\n",
    "    # max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "    k_h = 3\n",
    "    k_w = 3\n",
    "    s_h = 2\n",
    "    s_w = 2\n",
    "    padding = 'VALID'\n",
    "    maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "\n",
    "    # fc6, 4096\n",
    "    fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "    fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "    flat5 = tf.reshape(maxpool5, [-1, int(np.prod(maxpool5.get_shape()[1:]))]) \n",
    "    fc6 = tf.nn.relu(tf.matmul(flat5, fc6W) + fc6b)\n",
    "\n",
    "\n",
    "    # fc7, 4096\n",
    "    fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "    fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "    fc7 = tf.nn.relu(tf.matmul(fc6, fc7W) + fc7b)\n",
    "\n",
    "    if feature_extract:\n",
    "        return fc7\n",
    "\n",
    "    # fc8, 1000\n",
    "    fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
    "    fc8b = tf.Variable(net_data[\"fc8\"][1])\n",
    "\n",
    "    \n",
    "    logits = tf.matmul(fc7, fc8W) + fc8b\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Alexnet on TrafficSign dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Time: 735.362 seconds\n",
      "Validation Loss = 0.5165088984584668\n",
      "Validation Accuracy = 0.8678414096870234\n",
      "\n",
      "Epoch 2\n",
      "Time: 687.318 seconds\n",
      "Validation Loss = 0.3434311076440162\n",
      "Validation Accuracy = 0.9116624159471671\n",
      "\n",
      "Epoch 3\n",
      "Time: 670.553 seconds\n",
      "Validation Loss = 0.27088368554435543\n",
      "Validation Accuracy = 0.9310611330317662\n",
      "\n",
      "Epoch 4\n",
      "Time: 676.075 seconds\n",
      "Validation Loss = 0.22128087872054406\n",
      "Validation Accuracy = 0.9448179921398889\n",
      "\n",
      "Epoch 5\n",
      "Time: 659.200 seconds\n",
      "Validation Loss = 0.188459708156924\n",
      "Validation Accuracy = 0.953319421898168\n",
      "\n",
      "Epoch 6\n",
      "Time: 758.689 seconds\n",
      "Validation Loss = 0.16934869475780753\n",
      "Validation Accuracy = 0.9557152793832905\n",
      "\n",
      "Epoch 7\n",
      "Time: 598.717 seconds\n",
      "Validation Loss = 0.15094373613510742\n",
      "Validation Accuracy = 0.9626709946903179\n",
      "\n",
      "Epoch 8\n",
      "Time: 697.709 seconds\n",
      "Validation Loss = 0.13986919338459558\n",
      "Validation Accuracy = 0.9672308524615504\n",
      "\n",
      "Epoch 9\n",
      "Time: 608.866 seconds\n",
      "Validation Loss = 0.13281788772275951\n",
      "Validation Accuracy = 0.9659169951540323\n",
      "\n",
      "Epoch 10\n",
      "Time: 769.854 seconds\n",
      "Validation Loss = 0.12658033293914975\n",
      "Validation Accuracy = 0.9653759950537136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "#import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "nb_classes = 43\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "with open('./train.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['features'], data['labels'], test_size=0.33, random_state=0)\n",
    "\n",
    "features = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "labels = tf.placeholder(tf.int64, None)\n",
    "resized = tf.image.resize_images(features, (227, 227))\n",
    "\n",
    "# Returns the second final layer of the AlexNet model,\n",
    "# this allows us to redo the last layer for the traffic signs\n",
    "# model.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "logits = tf.matmul(fc7, fc8W) + fc8b\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(loss_op, var_list=[fc8W, fc8b])\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "preds = tf.arg_max(logits, 1)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
    "\n",
    "\n",
    "def eval_on_data(X, y, sess):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for offset in range(0, X.shape[0], batch_size):\n",
    "        end = offset + batch_size\n",
    "        X_batch = X[offset:end]\n",
    "        y_batch = y[offset:end]\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={features: X_batch, labels: y_batch})\n",
    "        total_loss += (loss * X_batch.shape[0])\n",
    "        total_acc += (acc * X_batch.shape[0])\n",
    "\n",
    "    return total_loss/X.shape[0], total_acc/X.shape[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # training\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        t0 = time.time()\n",
    "        for offset in range(0, X_train.shape[0], batch_size):\n",
    "            end = offset + batch_size\n",
    "            sess.run(train_op, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
    "\n",
    "        val_loss, val_acc = eval_on_data(X_val, y_val, sess)\n",
    "        print(\"Epoch\", i+1)\n",
    "        print(\"Time: %.3f seconds\" % (time.time() - t0))\n",
    "        print(\"Validation Loss =\", val_loss)\n",
    "        print(\"Validation Accuracy =\", val_acc)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "Roundabout mandatory: 0.052\n",
      "Keep right: 0.049\n",
      "No entry: 0.041\n",
      "Wild animals crossing: 0.038\n",
      "Go straight or left: 0.038\n",
      "\n",
      "Image 1\n",
      "Speed limit (20km/h): 0.072\n",
      "Keep left: 0.061\n",
      "No vechiles: 0.059\n",
      "Speed limit (70km/h): 0.057\n",
      "Right-of-way at the next intersection: 0.053\n",
      "\n",
      "Time: 6.300 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "sign_names = pd.read_csv('signnames.csv')\n",
    "nb_classes = 43\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "# NOTE: By setting `feature_extract` to `True` we return\n",
    "# the second to last layer.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "# TODO: Define a new fully connected layer followed by a softmax activation to classify\n",
    "# the traffic signs. Assign the result of the softmax activation to `probs` below.\n",
    "# HINT: Look at the final layer definition in alexnet.py to get an idea of what this\n",
    "# should look like.\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)  # use this shape for the weight matrix\n",
    "\n",
    "# fc8, 43\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "\n",
    "# logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "logits = tf.matmul(fc7, fc8W) + fc8b\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = imread(\"construction.jpg\").astype(np.float32)\n",
    "im1 = im1 - np.mean(im1)\n",
    "\n",
    "im2 = imread(\"stop.jpg\").astype(np.float32)\n",
    "im2 = im2 - np.mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(probs, feed_dict={x: [im1, im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = np.argsort(output)[input_im_ind, :]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (sign_names.iloc[inds[-1 - i]][1], output[input_im_ind, inds[-1 - i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time() - t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
